\documentclass{article}
\usepackage[dutch]{babel}
\usepackage{graphicx}

%\usepackage{tikz}
%\usepackage{tkz-graph}
%\usetikzlibrary{babel,graphdrawing,graphs,arrows.meta,shapes.misc,chains,positioning,shapes,quotes,automata,bending}
%\usegdlibrary{trees}
%\usegdlibrary{layered}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[a4paper]{geometry}
%\usepackage{fullpage}
\usepackage{etoolbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{adjustbox}
%\usepackage{minted}
\usepackage{enumerate}
\usepackage{morefloats}
\usepackage{float}
\usepackage{lmodern}
%\usepackage[stretch=10]{microtype}


\makeatletter
\patchcmd{\maketitle}{\@fnsymbol}{\@alph}{}{}  % Footnote numbers from symbols to small letters
\makeatother

\title{Huiswerk 6\\ \large{Statistisch Redeneren UvA-2015}}
\author{Jelte Fennema\thanks{Student nummer 10183159} ~\& Bas van den
Heuvel\thanks{Student nummer 10343725}}

\date{\today}

\begin{document}
\maketitle

\section{Support Vector Machines}
Support Vector Machines (SVMs) zijn niet-probabilistische binaire lineaire
classificators. De SVMs kunnen ook niet-lineaire problemen oplossen, en het is
mogelijk ze toe te passen voor problemen met meer dan twee classes.

De lineaire SVM probeert een hyperplane te vinden die de datapunten van
verschillende classes scheidt met een zo groot mogelijke marge. Als zo'n
hyperplane niet gevonden kan worden, kan gebruik gemaakt worden van een ``soft
margin'', die misclassificaties toe laat. Hierbij bepaalt constante $C$ de
trade-off tussen een grote marge en een kleine mate van misclassificatie.

Om niet-lineaire problemen op te lossen, transformeer je de originele datapunten
d.m.v. de kernel trick naar een hogere dimensie, waar een lineaire scheiding met
maximale marge gevonden kan worden. Deze hyperplane is lineair in een hogere
dimensie, waar deze in de originele ruimte niet lineair lijkt. De kernel die
hierbij gebruikt wordt is niet-lineair.

Om met SVMs meerdere classes dan twee te onderscheiden, maak je gebruik van
meerdere SVMs, waarbij de output met elkaar wordt vergeleken. De beste output
bepaalt dan de class waar het datapunt in wordt geclassificeerd. Hierbij zijn
twee opties mogelijk: \textit{one-versus-one} en \textit{one-versus-all}. Bij de
eerste vergelijk je omstebeurt de SVMs van de verschillende classes met elkaar
(in paren van twee), de class die het vaakste wint wordt gekozen als uitkomst.
Bij de tweede wordt per class een SVM gemaakt die de data scheidt tussen
\textit{deze class} en \textit{andere class}. De SVM met de hoogste confidence
bepaalt bij deze strategie de uitkomst.

\section{Kleurtjes}

\subsection{Data}
De data die gebruikt is bestaat uit een lijst met spectogrammen, met daarbij als
label de kleur die bij dat spectogram hoort.

\subsection{Procedure}
Allereerst wordt de data opgesplitst in 2 delen, de leer set en de test set.
Dan wordt doormiddel van een gridsearch de beste set parameters gezocht voor
gebruik door de svm met dataset. Dit wordt gedaan door binnen de leerset de
paramaters te kiezen die het beste resultaat geven. Dit wordt gedaan door
k-crossvalidation met $k=3$. Dit betekent dat de dataset (de leer data) wordt
opgesplitst in 3 delen en voor elk van die delen gekeken wordt met welke
parameters het beste resultaat ontstaant waneer er getraind wordt op de
overgebleven 2 delen.

Als laatste worden de classes van de test geraden aan de hand van de clasifier
die geleerd is met de leerdata en de daarbij passende beste parameters. Deze
geraden classes worden vergeleken met de daadwerkelijke classes en uiteengezet
in een confusion matrix en de totale accuracy wordt ook bepaald.


\subsection{Resultaten}
De SVM met geschatte parameters zijn aanzienlijk beter. De resultaten liggen
rond de 77\% nauwkeurigheid. Wel klaagde \textit{sklearn} dat de kleinste class
maar één datapunt heeft. Bij nagaan van de confusion matrices en de dataset
zelf, kwamen we erachter dat het hierbij ging om het enkele datapunt met de
kleur ``roze''. Na uitgebreid testen van de SVM zónder dit datapunt, komen we
tot de conclusie dat dit geen invloed heeft op het eindresultaat, aangezien ook
hierbij de resultaten rond de 77\% nauwkeurigheid liggen. Zonder de grijze
datapunten, waarvan er maar drie voorkomen, komen we uit op 78\% nauwkeurigheid.
Zonder de zwarte datapunten (vier stuks), verhoogt de nauwkeurigheid naar 80\%.
Het aantal leervoorbeelden per class lijkt dus wel degelijk belangrijk te zijn.

\end{document}
